{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing datasets that have been transformed in Part 1\n",
    "https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Importing our modified datasets from RentHop_Interest_Part1:\n",
    "train = pd.read_csv('data/train_transformed.csv')\n",
    "test = pd.read_csv('data/test_transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>price</th>\n",
       "      <th>interest_level</th>\n",
       "      <th>elevator_fl</th>\n",
       "      <th>hardwood_fl</th>\n",
       "      <th>cats_fl</th>\n",
       "      <th>dogs_fl</th>\n",
       "      <th>doorman_fl</th>\n",
       "      <th>dishwasher_fl</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_categories_night</th>\n",
       "      <th>hour_categories_morning</th>\n",
       "      <th>hour_categories_afternoon</th>\n",
       "      <th>hour_categories_evening</th>\n",
       "      <th>borough_Bronx</th>\n",
       "      <th>borough_Brooklyn</th>\n",
       "      <th>borough_Manhattan</th>\n",
       "      <th>borough_Queens</th>\n",
       "      <th>borough_Staten_Island</th>\n",
       "      <th>borough_outside</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.482820</td>\n",
       "      <td>1.311964</td>\n",
       "      <td>-0.049358</td>\n",
       "      <td>medium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.357225</td>\n",
       "      <td>0.411224</td>\n",
       "      <td>0.106361</td>\n",
       "      <td>low</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.357225</td>\n",
       "      <td>-0.489516</td>\n",
       "      <td>-0.058834</td>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.357225</td>\n",
       "      <td>-0.489516</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357225</td>\n",
       "      <td>2.212704</td>\n",
       "      <td>-0.027248</td>\n",
       "      <td>low</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bathrooms  bedrooms     price interest_level  elevator_fl  hardwood_fl  \\\n",
       "0   0.482820  1.311964 -0.049358         medium            0            0   \n",
       "1  -0.357225  0.411224  0.106361            low            1            0   \n",
       "2  -0.357225 -0.489516 -0.058834           high            0            1   \n",
       "3  -0.357225 -0.489516 -0.031985            low            0            1   \n",
       "4  -0.357225  2.212704 -0.027248            low            0            0   \n",
       "\n",
       "   cats_fl  dogs_fl  doorman_fl  dishwasher_fl  ...  hour_categories_night  \\\n",
       "0        0        0           0              0  ...                      0   \n",
       "1        1        1           1              0  ...                      0   \n",
       "2        0        0           0              1  ...                      1   \n",
       "3        0        0           0              0  ...                      1   \n",
       "4        0        0           0              0  ...                      1   \n",
       "\n",
       "   hour_categories_morning  hour_categories_afternoon  \\\n",
       "0                        1                          0   \n",
       "1                        0                          1   \n",
       "2                        0                          0   \n",
       "3                        0                          0   \n",
       "4                        0                          0   \n",
       "\n",
       "   hour_categories_evening  borough_Bronx  borough_Brooklyn  \\\n",
       "0                        0              0                 1   \n",
       "1                        0              0                 0   \n",
       "2                        0              0                 0   \n",
       "3                        0              0                 0   \n",
       "4                        0              0                 0   \n",
       "\n",
       "   borough_Manhattan  borough_Queens  borough_Staten_Island  borough_outside  \n",
       "0                  0               0                      0                0  \n",
       "1                  1               0                      0                0  \n",
       "2                  1               0                      0                0  \n",
       "3                  1               0                      0                0  \n",
       "4                  1               0                      0                0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the evaluation criteria\n",
    "\n",
    "The test dataset will be evaluated using the multi-class logarithmic loss. Our submission will consist of probabilities for each class (in this case its the probability that the interest level for each listing is 'low', 'medium' or 'high').  Below is the formula for the multi-class logarithmic loss:<br>\n",
    "![title](drawings/multiclass_logloss.jpg)<br>\n",
    "\n",
    "N is the number of listings in the dataset, M is the number of classes, yij is 1 if listing i belongs to class j and is 0 if it doesn't, log is the natural log, and pij is the probability that listing i belongs to class j.<br> \n",
    "\n",
    "<b>Notes</b>:<br><br>\n",
    "The three probabilities for each listing don't need to sum to 1 in the submission, but they will be rescaled to sum to 1 after the submission.<br>\n",
    "If the predicted probability for a listing i that actually belongs to class j is extremely low, the log of that value will approach negative infinity and will result in the overall error to approach infinity, even if the other listings have good predictions. In order to not have a few bad predictions take over the entire error, the predicted errors will be replaced with max((min(pij,1-1e-15)1e-15). This emposes a ceiling and floor on the probability so that we never take the log of a value less than 1e-15 ore more than 1-1e-15. <br>\n",
    "\n",
    "I am going to first implement my own multi-class logarithmic loss scoring function to ensure that I know the details of how it can be implemented and don't need to rely upon the scikit learn package. When I actually begin doing model selection and hyperparameter tuning I will be using scikit's 'neg_log_loss' scoring function out of convenience. <br>\n",
    "\n",
    "<br>\n",
    "We can gauge how well a model is doing by comparing it to a \"dumb\" baseline classifier. If we had just predicted a probability of 0.333 for each class for each listing (ie equal probability that each listing is of 'low', 'medium', or 'high' interest level), we would get a multiclass log loss of 1.10. We know this b/c the multiclass log loss in this case is equal to 1/N * N * log(0.33) which is simplified to log(0.33) which is equivalent to 1.10. <br>\n",
    "\n",
    "If we take into account the prevelance of each class in the train dataset, we can make an even better \"dumb\" baseline classifier if, for each listing, we set the probability of each class equal to its prevalance in the dataset. So in this case, listings with a 'low' interest level were ~69.468% of the listings. Listings with a 'medium' interest level were ~22.753% of the listings. Listings with a 'high' interest level were ~7.779% of the listings. We can set the prediction for the 'low' interest level to 0.69468, the prediction for the 'medium' interest level to 0.22753 and the prediction for the 'high' interest level to 0.07779 for every listing. \n",
    "\n",
    "https://stats.stackexchange.com/questions/276067/whats-considered-a-good-log-loss\n",
    "https://medium.com/@fzammito/whats-considered-a-good-log-loss-in-machine-learning-a529d400632d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree predictions:\n",
      " [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "decision tree 2 predictions:\n",
      " [[0.14285714 0.71428571 0.14285714]\n",
      " [0.14285714 0.71428571 0.14285714]\n",
      " [0.23123123 0.32732733 0.44144144]\n",
      " ...\n",
      " [0.23123123 0.32732733 0.44144144]\n",
      " [0.07692308 0.42307692 0.5       ]\n",
      " [0.         0.98974359 0.01025641]]\n",
      "logistic regression predictions:\n",
      " [[0.13749065 0.52994628 0.33256307]\n",
      " [0.14623071 0.41490856 0.43886073]\n",
      " [0.07851478 0.60096311 0.3205221 ]\n",
      " ...\n",
      " [0.13454349 0.44413792 0.42131859]\n",
      " [0.18526336 0.44249825 0.37223838]\n",
      " [0.01256163 0.90515997 0.0822784 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = train.drop(['interest_level'], axis=1)\n",
    "y = train['interest_level']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=2)\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "prob_target_dt = dtc.predict_proba(X_test)\n",
    "print('decision tree predictions:\\n', prob_target_dt)\n",
    "\n",
    "#In this decision tree, I will try to limit overfitting (and thus increase the bias in order to reduce the variance of the model)\n",
    "dtc_2 = DecisionTreeClassifier(max_depth = 10, min_samples_leaf=15)\n",
    "dtc_2.fit(X_train, y_train)\n",
    "prob_target_dt_2 = dtc_2.predict_proba(X_test)\n",
    "print('decision tree 2 predictions:\\n', prob_target_dt_2)\n",
    "\n",
    "#Using the one-vs-all (also known as one-vs-rest (ovr)) method of multi-class classification.\n",
    "#\n",
    "lgr = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
    "lgr.fit(X_train, y_train)\n",
    "predictions = lgr.predict(X_test)\n",
    "prob_target_lgr = lgr.predict_proba(X_test)\n",
    "print('logistic regression predictions:\\n', prob_target_lgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe consisting of 4 columns: 3 columns for each of the probabilities for each listing and 1 column for the actual class of each listing.\n",
    "y_test_df = y_test.to_frame()\n",
    "y_test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "prediction_probabilities_dt= pd.DataFrame(data=prob_target_dt,\n",
    "                 columns= ['high', 'low', 'medium'])\n",
    "prob_target_dt = pd.concat([prediction_probabilities_dt, y_test_df],axis=1)\n",
    "\n",
    "prediction_probabilities_dt_2= pd.DataFrame(data=prob_target_dt_2,\n",
    "                 columns= ['high', 'low', 'medium'])\n",
    "prob_target_dt_2 = pd.concat([prediction_probabilities_dt_2, y_test_df],axis=1)\n",
    "\n",
    "prediction_probabilities_lgr= pd.DataFrame(data=prob_target_lgr,\n",
    "                 columns= ['high', 'low', 'medium'])\n",
    "prob_target_lgr = pd.concat([prediction_probabilities_lgr, y_test_df],axis=1)\n",
    "\n",
    "\n",
    "#Creating dataframe of predictions for dumb baseline classifier\n",
    "#Creating each column starting with numpy arrays\n",
    "low = np.zeros((prob_target_lgr.shape[0],1), dtype=float)\n",
    "low += 0.69468\n",
    "medium = np.zeros((prob_target_lgr.shape[0],1), dtype=float)\n",
    "medium += 0.22753\n",
    "high = np.zeros((prob_target_lgr.shape[0],1), dtype=float)\n",
    "high += 0.07779\n",
    "\n",
    "predictions_dumb = np.concatenate((high,low,medium), axis=1)\n",
    "#print(predictions_dumb)\n",
    "\n",
    "prediction_probabilities_dumb= pd.DataFrame(data=predictions_dumb,\n",
    "                 columns= ['high', 'low', 'medium'])\n",
    "prob_target_dumb = pd.concat([prediction_probabilities_dumb, y_test_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My personal implementation for evaluating the multiclass log loss on a set of predictions:\n",
    "def row_multiclasslogloss(prob_target_row):\n",
    "    unit_log_loss = 0\n",
    "    eps = 1e-15\n",
    "    if prob_target_row.loc['interest_level'] == 'low':\n",
    "        a = prob_target_row['low']\n",
    "        #Note that numpy's clip() function below returns eps if a is less than eps and 1-eps if a is more than 1-eps\n",
    "        #This makes sure that prob_target_row['low'] is kept between 1e-15 and 1-e-15.\n",
    "        #Otherwise, if for one datapoint prob_target_row['low'] was ~0, then the log of it would be -inf.\n",
    "        #This way extreme values don't completely control overall value.\n",
    "        b = np.clip(a, eps, 1 - eps)\n",
    "        unit_log_loss = 1*np.log(b)\n",
    "        return unit_log_loss\n",
    "    if prob_target_row.loc['interest_level'] == 'medium':\n",
    "        a = prob_target_row['medium']\n",
    "        b = np.clip(a, eps, 1 - eps)\n",
    "        unit_log_loss = 1*np.log(b)\n",
    "        return unit_log_loss\n",
    "    if prob_target_row.loc['interest_level'] == 'high':\n",
    "        a = prob_target_row['high']\n",
    "        b = np.clip(a, eps, 1 - eps)\n",
    "        unit_log_loss = 1*np.log(b)\n",
    "        return unit_log_loss\n",
    "    \n",
    "def overall_multiclasslogloss(prob_target):\n",
    "    loss_series =  prob_target.apply(row_multiclasslogloss, axis=1)\n",
    "    loss_series_sum = loss_series.sum()*(-1/prob_target.shape[0])\n",
    "    \n",
    "    return loss_series_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiclass log loss for decision tree: 12.490455074603624\n",
      "multiclass log loss for decision tree 2: 0.9511805831094223\n",
      "multiclass log loss for logistic regression: 0.6864907218337554\n",
      "multiclass log loss for dumb baseline classifier: 0.7889918473333496\n"
     ]
    }
   ],
   "source": [
    "loss_amount_dt = overall_multiclasslogloss(prob_target_dt)\n",
    "print('multiclass log loss for decision tree:',loss_amount_dt)\n",
    "loss_amount_dt_2 = overall_multiclasslogloss(prob_target_dt_2)\n",
    "print('multiclass log loss for decision tree 2:',loss_amount_dt_2)\n",
    "loss_amount_lgr =  overall_multiclasslogloss(prob_target_lgr)\n",
    "print('multiclass log loss for logistic regression:', loss_amount_lgr)\n",
    "loss_amount_dumb =  overall_multiclasslogloss(prob_target_dumb)\n",
    "print('multiclass log loss for dumb baseline classifier:', loss_amount_dumb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observations</b>:<br>\n",
    "1.) <br> \n",
    "The first decision tree has such high error b/c it is WAY overfitting (I added no parameters to limit this).<br>\n",
    "This decision tree's predicted probabilities for each listing consist of 1 for the predicted class and 0s for the classes not predicted.This is another sign that the model is overfitting b/c this indicates that each leaf in the tree exclusively contains listings of one class.<br>\n",
    "2.)<br> This second tree has much lower error b/c I set parameters that limit the depth of the tree resulting in less overfitting. <br>\n",
    "3.)<br>The simple logistic regression model beats the dumb baseline classifier which is a good sign.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Selection & Hyperparameter Tuning\n",
    "\n",
    "Note that from here on out I will be using sklearn's implemention of multiclass logarithmic loss rather than my own out of convenience. \n",
    "\n",
    "I will be using the following algorithms:\n",
    "\n",
    "<b>Generalized linear models</b>:<br>\n",
    "1.) Simple logistic regression <br>\n",
    "2.) Logistic regression w/ L1 Regularization-> not converged and run time is WAY too long.<br>\n",
    "3.) Logistic regression w/ L2 Regularization <br>\n",
    "4.) Logistic regression w/ ElasticNet Regularization -> not converged and run time is WAY too long.<br>\n",
    "\n",
    "<b>Nearest-Neighbors model</b>:<br>\n",
    "1.) KNeighborsClassifier -> ran but bad results (probably need to do some feature selection beforehand).<br>\n",
    "\n",
    "<b>Tree-based models</b>: <br>\n",
    "1.) Random Forest classification (tree-based ensemble method)<br>\n",
    "2.) AdaBoost classification (tree-based ensemble method)<br>\n",
    "3.) Gradient Boosting classification (tree-based ensemble method)<br>\n",
    "4.) XGBoost classification (tree-based ensemble method)<br>\n",
    "\n",
    "<b>Support Vector Machine model</b>:<br>\n",
    "1.) Support Vector Classification -> run time too long\n",
    "\n",
    "<b>Ensemble algorithms</b>:<br>\n",
    "1.) Voting classifier<br>\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumb baseline classifier\n",
    "Like I did with my own implementation of multi-class log loss above (in that case it was calculated for half the train dataset b/c I did a train-test split on the train dataset), I will calculate what the log loss would be for the dumb baseline classifer on the entire train dataset so that we have a baseline to compare to when evaluating how effective each model is. Note that the log loss should be very close to what I calculated above (0.789). This is b/c the log loss is an average across the dataset so it doesn't depend on the size of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for the \"dumb\" baseline classifier: 0.7885769114648241\n"
     ]
    }
   ],
   "source": [
    "#Do the dumb baseline classifier:\n",
    "from sklearn.metrics import log_loss\n",
    "#Creating dataframe of predictions for dumb baseline classifier\n",
    "#Creating each column starting with numpy arrays\n",
    "low = np.zeros((train.shape[0],1), dtype=float)\n",
    "low += 0.69468\n",
    "medium = np.zeros((train.shape[0],1), dtype=float)\n",
    "medium += 0.22753\n",
    "high = np.zeros((train.shape[0],1), dtype=float)\n",
    "high += 0.07779\n",
    "\n",
    "predictions_dumb = np.concatenate((high,low,medium), axis=1)\n",
    "#print(predictions_dumb)\n",
    "\n",
    "prediction_probabilities_dumb= pd.DataFrame(data=predictions_dumb,\n",
    "                 columns= ['high', 'low', 'medium'])\n",
    "prob_target_dumb = pd.concat([prediction_probabilities_dumb, train['interest_level']],axis=1)\n",
    "\n",
    "dumb_baseline_logloss = log_loss(train['interest_level'], prediction_probabilities_dumb)\n",
    "print('log loss for the \"dumb\" baseline classifier:', dumb_baseline_logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1: liblinear\n",
    "l2: liblinear, newton-cg, sag, lbfgs\n",
    "elasticnet: saga\n",
    "none: newton-cg, sag, lbfgs, saga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49352\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='none',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "{'solver': 'lbfgs'}\n",
      "log loss for simple logistic regression: 0.6811067236543596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "\n",
    "KF = KFold(n_splits=5, shuffle=True, random_state=14)\n",
    "\n",
    "#Redefine so that X_train is for the entire training dataset (above we only used half)\n",
    "#This will take longer to run\n",
    "X_train = train.drop(['interest_level'], axis=1)\n",
    "y_train = train['interest_level']\n",
    "\n",
    "#Had to increase # of iterations from 100 for any of this model to converge\n",
    "lgr = LogisticRegression(multi_class='ovr', penalty='none', max_iter=10000)\n",
    "\n",
    "#params_lgr = {\"solver\": ['newton-cg',  'sag', 'lbfgs', 'saga']}\n",
    "params_lgr = {\"solver\": ['lbfgs','newton-cg']}\n",
    "print(len(X_train))\n",
    "grid_lgr = GridSearchCV(lgr, param_grid=params_lgr, scoring='neg_log_loss', cv=KF)\n",
    "grid_lgr.fit(X_train, y_train)\n",
    "\n",
    "lgr_best_estimator = grid_lgr.best_estimator_\n",
    "lgr_best_params = grid_lgr.best_params_\n",
    "lgr_best_score = -1*(grid_lgr.best_score_)\n",
    "\n",
    "print(lgr_best_estimator)\n",
    "print(lgr_best_params)\n",
    "print('log loss for simple logistic regression:', lgr_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1 penalty\n",
    "#check to see if saved\n",
    "\n",
    "<b> Run-time is WAY too long. </b><br>\n",
    "Need to look into strategies to decrease the run-time.<br>\n",
    "https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgr_l1 = LogisticRegression(multi_class='ovr', penalty=\"l1\", solver='liblinear', max_iter=10000)\n",
    "\n",
    "#params_lgr_l1 = {\"C\": [50, 100, 500]}\n",
    "\n",
    "#grid_lgr_l1 = GridSearchCV(lgr_l1, param_grid=params_lgr_l1, scoring='neg_log_loss', cv=KF)\n",
    "#grid_lgr_l1.fit(X_train, y_train)\n",
    "\n",
    "#lgr_l1_best_estimator = grid_lgr_l1.best_estimator_\n",
    "#lgr_l1_best_params = grid_lgr_l1.best_params_\n",
    "#lgr_l1_best_score = -1*(grid_lgr_l1.best_score_)\n",
    "\n",
    "#print(lgr_l1_best_estimator)\n",
    "#print(lgr_l1_best_params)\n",
    "#print('log loss for logistic regression w/ L1 Regularization:', lgr_l1_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=6000, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "{'C': 6000, 'solver': 'lbfgs'}\n",
      "log loss for logistic regression w/ l2 penalty: 0.6808655800944188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lgr_l2 = LogisticRegression(multi_class='ovr', penalty=\"l2\")\n",
    "\n",
    "#Spent a lot of time tuning this, 'lbfgs' seems to work better and to actually be able to converge.\n",
    "params_lgr_l2 = {\"C\": [3500, 5000, 6000],\n",
    "                'solver': ['lbfgs']}\n",
    "\n",
    "grid_lgr_l2 = GridSearchCV(lgr_l2, param_grid=params_lgr_l2, scoring='neg_log_loss', cv=KF)\n",
    "grid_lgr_l2.fit(X_train, y_train)\n",
    "\n",
    "lgr_l2_best_estimator = grid_lgr_l2.best_estimator_\n",
    "lgr_l2_best_params = grid_lgr_l2.best_params_\n",
    "lgr_l2_best_score = -1*(grid_lgr_l2.best_score_)\n",
    "\n",
    "print(lgr_l2_best_estimator)\n",
    "print(lgr_l2_best_params)\n",
    "print('log loss for logistic regression w/ l2 penalty:', lgr_l2_best_score)\n",
    "#print(grid_lgr_l2.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "<b> Run-time is WAY too long (likely from the L1 regularization term). </b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgr_en = LogisticRegression(multi_class='ovr', penalty=\"elasticnet\", solver='saga')\n",
    "\n",
    "#params_lgr_en = {\"C\": [1000, 5000, 10000],\n",
    "#                'l1_ratio': [0.25, 0.5, 0.9]}\n",
    "\n",
    "#grid_lgr_en = GridSearchCV(lgr_en, param_grid=params_lgr_en, scoring='neg_log_loss', cv=KF)\n",
    "#grid_lgr_en.fit(X_train, y_train)\n",
    "\n",
    "#lgr_en_best_estimator = grid_lgr_en.best_estimator_\n",
    "#lgr_en_best_params = grid_lgr_en.best_params_\n",
    "#lgr_en_best_score = -1*(grid_lgr_en.best_score_)\n",
    "\n",
    "#print(lgr_en_best_estimator)\n",
    "#print(lgr_en_best_params)\n",
    "#print('log loss for logistic regression w/ ElasticNet:', lgr_en_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=5,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=20,\n",
      "                       n_jobs=None, oob_score=False, random_state=7, verbose=0,\n",
      "                       warm_start=False)\n",
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 20}\n",
      "0.6703905305945391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=7)\n",
    "\n",
    "params_rfc ={\n",
    "        \"n_estimators\": [10, 20],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [2, 5, 10],\n",
    "        \"max_features\": [\"log2\", \"sqrt\"],\n",
    "        \"min_samples_leaf\": [1, 5, 8],\n",
    "        \"min_samples_split\": [2, 3, 5]\n",
    "}\n",
    "\n",
    "\n",
    "grid_rfc = GridSearchCV(rfc, param_grid=params_rfc, scoring='neg_log_loss',return_train_score=False, cv=KF)\n",
    "grid_rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_best_model = grid_rfc.best_estimator_\n",
    "rfc_best_params = grid_rfc.best_params_\n",
    "rfc_best_score = -1*(grid_rfc.best_score_)\n",
    "\n",
    "print(rfc_best_model)\n",
    "print(rfc_best_params)\n",
    "print(rfc_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.05,\n",
      "                   n_estimators=30, random_state=65)\n",
      "{'algorithm': 'SAMME.R', 'learning_rate': 0.05, 'n_estimators': 30}\n",
      "0.8158580933459949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=65)\n",
    "\n",
    "params_ada ={\n",
    "        #was 100,200 but VERY long\n",
    "        \"n_estimators\": [30],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "        \"algorithm\": ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "grid_ada = GridSearchCV(ada, param_grid=params_ada, scoring='neg_log_loss',return_train_score=False, cv=KF)\n",
    "grid_ada.fit(X_train, y_train)\n",
    "\n",
    "ada_best_model = grid_ada.best_estimator_\n",
    "ada_best_params = grid_ada.best_params_\n",
    "ada_best_score = abs(grid_ada.best_score_)\n",
    "\n",
    "print(ada_best_model)\n",
    "print(ada_best_params)\n",
    "print(ada_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=10,\n",
      "                           max_features='sqrt', max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=8, min_samples_split=10,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=30,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=76, subsample=0.8, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "{'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 8, 'min_samples_split': 10, 'n_estimators': 30, 'subsample': 0.8}\n",
      "0.635702147144199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=76)\n",
    "\n",
    "#tuned\n",
    "params_gbm ={\n",
    "        \"loss\": ['deviance'],\n",
    "        \"n_estimators\": [30],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"subsample\": [0.8],\n",
    "        \"max_depth\": [10],\n",
    "        \"max_features\": [\"sqrt\"],\n",
    "        \"min_samples_leaf\": [8],\n",
    "        \"min_samples_split\": [10]\n",
    "}\n",
    "\n",
    "\n",
    "grid_gbm = GridSearchCV(gbm, param_grid=params_gbm, scoring='neg_log_loss',return_train_score=False, cv=KF)\n",
    "grid_gbm.fit(X_train, y_train)\n",
    "\n",
    "gbm_best_model = grid_gbm.best_estimator_\n",
    "gbm_best_params = grid_gbm.best_params_\n",
    "gbm_best_score = abs(grid_gbm.best_score_)\n",
    "\n",
    "print(gbm_best_model)\n",
    "print(gbm_best_params)\n",
    "print(gbm_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classification\n",
    "https://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, eval_metric='logloss',\n",
      "              gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, num_class=3, objective='multi:softprob',\n",
      "              random_state=17, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "              seed=None, silent=None, subsample=0.8, verbosity=1)\n",
      "{'eval_metric': 'logloss', 'learning_rate': 0.1, 'n_estimators': 100, 'num_class': 3, 'objective': 'multi:softprob', 'subsample': 0.8}\n",
      "0.6447125322983399\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "from xgboost import XGBClassifier\n",
    "#https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "xgb = XGBClassifier(random_state =17)\n",
    "\n",
    "params_xgb ={\n",
    "        \"n_estimators\": [100],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"subsample\": [0.8],\n",
    "        \"objective\": [\"multi:softprob\"],\n",
    "        \"eval_metric\": [\"logloss\"],\n",
    "        \"num_class\": [3]\n",
    "}\n",
    "\n",
    "\n",
    "grid_xgb = GridSearchCV(xgb, param_grid=params_xgb, scoring='neg_log_loss',return_train_score=False, cv=KF)\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "xgb_best_model = grid_xgb.best_estimator_\n",
    "xgb_best_params = grid_xgb.best_params_\n",
    "xgb_best_score = abs(grid_xgb.best_score_)\n",
    "\n",
    "print(xgb_best_model)\n",
    "print(xgb_best_params)\n",
    "print(xgb_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest-Neighbors \n",
    "\n",
    "<b> Takes a VERY long time to run and does not give good results. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price                    0.241493\n",
      "nofee_fl                 0.073116\n",
      "number_of_photos         0.071503\n",
      "words_in_description     0.059147\n",
      "bedrooms                 0.046701\n",
      "hour_categories_night    0.044535\n",
      "hardwood_fl              0.041357\n",
      "num_features_listed      0.038714\n",
      "bathrooms                0.032052\n",
      "has_photos               0.031411\n",
      "dtype: float64\n",
      "Index(['price', 'nofee_fl', 'number_of_photos', 'words_in_description',\n",
      "       'bedrooms', 'hour_categories_night', 'hardwood_fl',\n",
      "       'num_features_listed', 'bathrooms', 'has_photos'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "feature_importance_series = pd.Series(index=X_train.columns, data=(rfc_best_model.feature_importances_)).sort_values(ascending=False)\n",
    "print(feature_importance_series.iloc[0:10])\n",
    "top_performing_features = feature_importance_series.index[0:10]\n",
    "print(top_performing_features)\n",
    "\n",
    "X_train = train[top_performing_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "                     weights='uniform')\n",
      "{'algorithm': 'ball_tree', 'n_neighbors': 10, 'p': 2, 'weights': 'uniform'}\n",
      "1.9833393740643022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "params_knn ={\n",
    "#                    \"n_neighbors\": [3,10,20],\n",
    "#                    \"weights\": [\"distance\", \"uniform\"],\n",
    "#                    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "#                    \"p\": [1,2]\n",
    "                    \"n_neighbors\": [10],\n",
    "                    \"weights\": [\"distance\", \"uniform\"],\n",
    "                    \"algorithm\": [\"ball_tree\"],\n",
    "                    \"p\": [2]\n",
    "\n",
    "}\n",
    "\n",
    "#params_knn ={\n",
    "#                    \"n_neighbors\": [5],\n",
    "#                    \"weights\": [\"uniform\"],\n",
    "#                    \"algorithm\": [\"auto\"],\n",
    "#                    \"p\": [2]\n",
    "#\n",
    "#}\n",
    "\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid=params_knn, scoring='neg_log_loss',return_train_score=False, cv=KF)\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "knn_best_model = grid_knn.best_estimator_\n",
    "knn_best_params = grid_knn.best_params_\n",
    "knn_best_score = abs(grid_knn.best_score_)\n",
    "\n",
    "print(knn_best_model)\n",
    "print(knn_best_params)\n",
    "print(knn_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine algorithms\n",
    "\n",
    "1.) SVC \n",
    "- May not work well for tens of thousands of datapoints\n",
    "- The multiclass case is handled with one-vs-one.\n",
    "- Support vector machine algorithms do NOT directly provide probabiliy estimates. However, SVC has a a 'probability' parameter that can set to True for probability estimates but this makes it takes longer to run. It is calculated with five-fold cross validation using Platt scaling which fits a logistic regression to the support vector classifier's scores. This is very computationally expensive. \n",
    "- <b>This algorithm was too computationally expensive and the training time was too long for this dataset </b> (tens of thousands of datapoints and required Platt scaling for probabilities). I may revisit this algorithm when I run this project with some form of cloud computing.\n",
    "\n",
    "2.) NuSVC\n",
    "- Similar to SVC but accepts slightly different parameters and has different mathematical formulation.\n",
    "\n",
    "3.) LinearSVC\n",
    "- Is the specific implementation of SVC with the kernal being linear. However, it is implemented with 'liblinear' (rather than  'libsvm' for SVC). This allows for more flexibility in the penalty options, more flexibility in the loss functions, and scales better to large #s of samples. \n",
    "- The multi-class case is handled with one-vs-rest technqiue (OVR). \n",
    "- <b> Does NOT have a parameter to return probability estimates for each of the classes </b>. For this project, our model is evaluated with the multiclass log loss which requires probability estimates for each of the classes. For this reason, I did NOT use the LinearSVC algorithm.\n",
    "\n",
    "Sources:<br>\n",
    "1.) https://scikit-learn.org/stable/modules/svm.html<br>\n",
    "2.) https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47<br>\n",
    "3.) http://web.mit.edu/6.034/wwwbob/svm.pdf<br>\n",
    "4.) http://cs229.stanford.edu/notes/cs229-notes3.pdf <br>\n",
    "5.) https://scikit-learn.org/stable/modules/svm.html#scores-and-probabilities <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVC \n",
    "\n",
    "\n",
    "#SVC = SVC(probability=True,gamma='auto', random_state=32)\n",
    "\n",
    "#tuned\n",
    "#params_SVC ={'C': [1],\n",
    "#             'kernel':['rbf']\n",
    "#}\n",
    "\n",
    "\n",
    "#grid_SVC = GridSearchCV(SVC, param_grid=params_SVC, scoring='neg_log_loss',return_train_score=False, cv=KF)\n",
    "#grid_SVC.fit(X_train, y_train)\n",
    "\n",
    "#SVC_best_model = grid_SVC.best_estimator_\n",
    "#SVC_best_params = grid_SVC.best_params_\n",
    "#SVC_best_score = abs(grid_SVC.best_score_)\n",
    "\n",
    "#print(SVC_best_model)\n",
    "#print(SVC_best_params)\n",
    "#print(SVC_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking w/ Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('xgboost',\n",
      "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                            colsample_bylevel=1,\n",
      "                                            colsample_bynode=1,\n",
      "                                            colsample_bytree=1,\n",
      "                                            eval_metric='logloss', gamma=0,\n",
      "                                            learning_rate=0.1, max_delta_step=0,\n",
      "                                            max_depth=3, min_child_weight=1,\n",
      "                                            missing=None, n_estimators=100,\n",
      "                                            n_jobs=1, nthread=None, num_class=3,\n",
      "                                            objective='multi:softprob',\n",
      "                                            random_state=17, reg...\n",
      "                                                         max_leaf_nodes=None,\n",
      "                                                         min_impurity_decrease=0.0,\n",
      "                                                         min_impurity_split=None,\n",
      "                                                         min_samples_leaf=8,\n",
      "                                                         min_samples_split=10,\n",
      "                                                         min_weight_fraction_leaf=0.0,\n",
      "                                                         n_estimators=30,\n",
      "                                                         n_iter_no_change=None,\n",
      "                                                         presort='auto',\n",
      "                                                         random_state=76,\n",
      "                                                         subsample=0.8,\n",
      "                                                         tol=0.0001,\n",
      "                                                         validation_fraction=0.1,\n",
      "                                                         verbose=0,\n",
      "                                                         warm_start=False))],\n",
      "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
      "                 weights=[0.25, 0.75])\n",
      "{'weights': [0.25, 0.75]}\n",
      "-0.638892547951329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "params_voting_ensemble = {'weights': [[0.50,0.50], [0.25, 0.75], [0.75, 0.25], [0,1], [1,0]]}\n",
    "\n",
    "#the estimator parameter needs to be a list of (string, estimator) tuples:\n",
    "estimators=[('xgboost', xgb_best_model), ('gbm', gbm_best_model)]\n",
    "\n",
    "#voting set to 'soft' allows for predicting class labels by weighting the probabilities, not the outcome\n",
    "voting_ensemble_model = VotingClassifier(estimators, voting='soft')\n",
    "\n",
    "grid_voting_ensemble = GridSearchCV(voting_ensemble_model, param_grid=params_voting_ensemble, scoring='neg_log_loss',return_train_score=False, cv=KF)\n",
    "grid_voting_ensemble.fit(X_train, y_train)\n",
    "\n",
    "voting_ensemble_best_model = grid_voting_ensemble.best_estimator_\n",
    "voting_ensemble_best_params = grid_voting_ensemble.best_params_\n",
    "voting_ensemble_best_score = grid_voting_ensemble.best_score_\n",
    "\n",
    "print(voting_ensemble_best_model)\n",
    "print(voting_ensemble_best_params)\n",
    "print(voting_ensemble_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_X = test.drop(['interest_level'],axis=1)\n",
    "\n",
    "#Using tuned random forest model\n",
    "model = rfc_best_model\n",
    "\n",
    "predictions_proba = model.predict_proba(test_dataset_X)\n",
    "prediction_probabilities_dt= pd.DataFrame(data=predictions_proba,\n",
    "                 columns= ['high', 'low', 'medium'])\n",
    "\n",
    "prediction_probabilities_dt_high = prediction_probabilities_dt['high']\n",
    "prediction_probabilities_dt_medium = prediction_probabilities_dt['medium']\n",
    "prediction_probabilities_dt_low = prediction_probabilities_dt['low']\n",
    "\n",
    "untransformed_test_dataset = pd.read_json('data/test.json')\n",
    "listing_id_series = untransformed_test_dataset['listing_id'].reset_index(drop=True)\n",
    "\n",
    "submission_dict = {'listing_id': listing_id_series, 'high': prediction_probabilities_dt_high,\n",
    "                  'medium': prediction_probabilities_dt_medium,\n",
    "                  'low': prediction_probabilities_dt_low}\n",
    "submission_df = pd.DataFrame(submission_dict)\n",
    "submission_df.to_csv('data/first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
